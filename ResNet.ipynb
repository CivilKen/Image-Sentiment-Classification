{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3_v5.ipnb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CivilKen/Image-Sentiment-Classification/blob/master/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfIoS3Wb4Ati",
        "colab_type": "code",
        "outputId": "91b8e82a-ba41-411e-9165-157d35a544d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "!ls \"/content/gdrive/My Drive/ML_homework/Colab Notebooks/hw3\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Code\t\t  df_valid_feature\tnor_df_valid_feature  train.csv\n",
            "df_test_feature   df_valid_label\tStoredModel\t      train_feature.csv\n",
            "df_train_feature  nor_df_test_feature\ttest.csv\n",
            "df_train_label\t  nor_df_train_feature\ttest_feature.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH7SGefu4GGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from skimage import io \n",
        "import matplotlib.pyplot as plt\n",
        "path = \"/content/gdrive/My Drive/ML_homework/Colab Notebooks/hw3/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lYGKrp9hmOB",
        "colab_type": "code",
        "outputId": "b3f76a06-39c9-490d-fa30-27ab06960b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#test GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvANH94h4PEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read dataframe \n",
        "df_train_feature = pd.read_pickle(path+\"df_train_feature\")\n",
        "df_valid_feature = pd.read_pickle(path+\"df_valid_feature\")\n",
        "df_test_feature = pd.read_pickle(path+\"df_test_feature\")\n",
        "df_train_label = pd.read_pickle(path+\"df_train_label\")\n",
        "df_valid_label = pd.read_pickle(path+\"df_valid_label\")\n",
        "#nor_df_train_feature = pd.read_pickle(path+\"nor_df_train_feature\")\n",
        "#nor_df_valid_feature = pd.read_pickle(path+\"nor_df_valid_feature\")\n",
        "#nor_df_test_feature = pd.read_pickle(path+\"nor_df_test_feature\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1dRWLvY4Tgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parse label data\n",
        "def one_hot(df_label,lenth):\n",
        "  #original input is dataframe\n",
        "  array_label = np.zeros([lenth,7])\n",
        "  for i in range(lenth):\n",
        "    idx = df_label.loc[i]\n",
        "    array_label[i, idx]=1\n",
        "  return array_label\n",
        "\n",
        "arr_train_label = one_hot(df_train_label, df_train_label.shape[0])\n",
        "arr_valid_label = one_hot(df_valid_label, df_valid_label.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCGtl6UI4Xas",
        "colab_type": "code",
        "outputId": "e3c51fb7-a75b-4082-a24c-982154fcd332",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "# define accuracy\n",
        "def accuracy(labels, pred):\n",
        "  #both labels and pred are in the shape of one-hot encoding\n",
        "  #both are in the format of array\n",
        "  accu=0\n",
        "  for i in range(len(labels)):\n",
        "    lab_val = np.argmax(labels[i,:])\n",
        "    pred_val = np.argmax(pred[i,:])\n",
        "    if lab_val==pred_val:\n",
        "      accu+=1\n",
        "  return accu/len(labels)\n",
        "'''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# define accuracy\\ndef accuracy(labels, pred):\\n  #both labels and pred are in the shape of one-hot encoding\\n  #both are in the format of array\\n  accu=0\\n  for i in range(len(labels)):\\n    lab_val = np.argmax(labels[i,:])\\n    pred_val = np.argmax(pred[i,:])\\n    if lab_val==pred_val:\\n      accu+=1\\n  return accu/len(labels)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaQ1_9BMryYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define hidden layer\n",
        "def dense(inputs, in_size, out_size, drop_rate, activation_function=None, Wname=None, banme=None):\n",
        "    # add one more layer and return the output of this layer\n",
        "    Weights = tf.Variable(tf.random_normal([in_size, out_size],stddev=0.01))\n",
        "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1)\n",
        "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
        "    #Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)\n",
        "    if activation_function is None:\n",
        "        outputs = Wx_plus_b\n",
        "    elif activation_function=='maxout':\n",
        "        outputs = tf.contrib.layers.maxout(Wx_plus_b,out_size)\n",
        "    else:\n",
        "        outputs = activation_function(Wx_plus_b)\n",
        "    outputs = tf.nn.dropout(x=outputs, rate=drop_rate)\n",
        "    return outputs, Weights, biases"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0fUEHZCH4DI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bottleneck_block(inputs, filters, in_size, train_mode):\n",
        "  \"\"\"\n",
        "  Implementation of the identity block.(Identity Shortcut)\n",
        "  It's used when the shortcut has the same dimension.\n",
        "  當sortcut的維度相等時使用\n",
        "  Arguments:\n",
        "  inputs: the input tensor, [batch, width, height, input_channel, filter_numbers]\n",
        "  filters: a int list with size of 3 elements, the filter number in each convolutional layer,\n",
        "           the last value would be output filter size\n",
        "  in_size: the input filter numbers\n",
        "  train_mode: a boolean, whether this step is in training mode\n",
        "  Returns:\n",
        "  X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
        "  \"\"\"\n",
        "  if in_size==filters[2]:\n",
        "    X_shortcut = inputs\n",
        "  elif in_size<filters[2]:\n",
        "    in_shape = tf.shape(inputs)\n",
        "    concat_zeros = tf.zeros([in_shape[0], in_shape[1], in_shape[2],  filters[2]-in_size])\n",
        "    X_shortcut = tf.concat([inputs, concat_zeros], axis=3)\n",
        "  #first\n",
        "  X = tf.layers.conv2d(inputs, filters[0], kernel_size=[1, 1], padding='same')\n",
        "  X = tf.layers.batch_normalization(X, axis=3, training=train_mode)\n",
        "  X = tf.nn.relu(X)\n",
        "  #second\n",
        "  X = tf.layers.conv2d(X, filters[1], kernel_size=[3, 3], padding='same')\n",
        "  #p.s. 為了不改變畫素大小，所以padding使用same\n",
        "  X = tf.layers.batch_normalization(X, axis=3, training=train_mode)\n",
        "  X = tf.nn.relu(X)\n",
        "  #third\n",
        "  X = tf.layers.conv2d(inputs, filters[2], kernel_size=[1, 1], padding='same')\n",
        "  #p.s. 當kernel sieze為1X1時，padding使用valid同樣不改變畫素大小\n",
        "  X = tf.layers.batch_normalization(X, axis=3, training=train_mode)\n",
        "  #final step\n",
        "  add = tf.add(X, X_shortcut)\n",
        "  add = tf.nn.relu(add)\n",
        "  return add"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pSTny2Y3fdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def building_block(inputs, filters, in_size, train_mode):\n",
        "  \"\"\"\n",
        "  Implementation of the identity block.(Identity Shortcut)\n",
        "  It's used when the shortcut has the same dimension.\n",
        "  當sortcut的維度相等時使用\n",
        "  Arguments:\n",
        "  inputs: the input tensor, [batch, width, height, input_channel, filter_numbers]\n",
        "  filters: a int list with size of 3 elements, the filter number in each convolutional layer,\n",
        "           the last value would be output filter size\n",
        "  in_size: the input filter numbers\n",
        "  train_mode: a boolean, whether this step is in training mode\n",
        "  Returns:\n",
        "  X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n",
        "  \"\"\"\n",
        "  if in_size==filters[2]:\n",
        "    X_shortcut = inputs\n",
        "  elif in_size<filters[2]:\n",
        "    in_shape = tf.shape(inputs)\n",
        "    concat_zeros = tf.zeros([in_shape[0], in_shape[1], in_shape[2],  filters[2]-in_size])\n",
        "    X_shortcut = tf.concat([inputs, concat_zeros], axis=3)\n",
        "  #first\n",
        "  X = tf.layers.conv2d(inputs, filters[0], kernel_size=[3, 3], padding='same')\n",
        "  X = tf.layers.batch_normalization(X, axis=3, training=train_mode)\n",
        "  X = tf.nn.relu(X)\n",
        "  #third\n",
        "  X = tf.layers.conv2d(inputs, filters[1], kernel_size=[3, 3], padding='same')\n",
        "  X = tf.layers.batch_normalization(X, axis=3, training=train_mode)\n",
        "  #final step\n",
        "  add = tf.add(X, X_shortcut)\n",
        "  add = tf.nn.relu(add)\n",
        "  return add"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLOdOkCW4sNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "InputNeurons = df_train_feature.shape[1]\n",
        "OutputNeurons = 7\n",
        "drop_rate = 0.5  # dropout parameter, drop how many percentage\n",
        "learning_rate = 0.0001\n",
        "beta = 0.001 #it's used in regularization as lamda\n",
        "printstep = 100\n",
        "traing_epochs = 50001\n",
        "\n",
        "# define placeholder for inputs to network\n",
        "rs = tf.placeholder(tf.float32) #drop rate\n",
        "ts = tf.placeholder(tf.bool) #training mode\n",
        "xs = tf.placeholder(tf.float32, [None, InputNeurons])\n",
        "ys = tf.placeholder(tf.float32, [None, OutputNeurons])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb7TXHqvp640",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_layer = tf.reshape(xs, [-1,48,48,1])\n",
        "conv1 = tf.layers.conv2d(inputs=input_layer, filters=64, kernel_size=[3,3], padding='same', activation=tf.nn.relu)\n",
        "bn1 = tf.layers.batch_normalization(conv1, axis=3, training=ts)\n",
        "rl1 = tf.nn.relu(bn1)\n",
        "pool1 = tf.layers.max_pooling2d(inputs=rl1, pool_size=[2,2], strides=2)\n",
        "res1 = bottleneck_block(pool1, [32,32,64], 64, ts)\n",
        "res2 = bottleneck_block(res1, [32,32,64], 64, ts)\n",
        "res3 = bottleneck_block(res2, [64,64,128], 64, ts)\n",
        "res4 = bottleneck_block(res3, [64,64,128], 128, ts)\n",
        "pool2 = tf.layers.average_pooling2d(inputs=res3, pool_size=[2,2], strides=2)\n",
        "pool_flat = tf.reshape(pool2, [-1, 12*12*128])\n",
        "[dense1, w1, b1] = dense(pool_flat, 12*12*128, 1024, rs, activation_function=tf.nn.relu)\n",
        "logits = tf.layers.dense(inputs=dense1, units=OutputNeurons)\n",
        "prediction = tf.nn.softmax(logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Fl3ImbpbYOA",
        "outputId": "072f99ae-bc02-45b8-80f6-930b8220fdaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#calculate loss function\n",
        "regularizers = tf.nn.l2_loss(w1)\n",
        "#loss = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), axis=1))\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=ys, logits=logits))\n",
        "loss = loss + beta*regularizers\n",
        "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "# start of the neural network\n",
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)\n",
        "\n",
        "TrainLoss=[]\n",
        "ValidLoss=[]\n",
        "epoch=[]\n",
        "for i in range(traing_epochs):\n",
        "    # training\n",
        "    idx = np.random.choice(np.arange(len(df_train_feature)), 64, replace=False)\n",
        "    x_train_batch = df_train_feature.loc[idx,:]\n",
        "    y_train_batch = arr_train_label[idx,:]\n",
        "    sess.run(train_step, feed_dict={xs: x_train_batch, ys: y_train_batch, rs: drop_rate, ts: True})\n",
        "    if i % printstep == 0:\n",
        "        # to see the step improvement\n",
        "        train_loss = sess.run(loss, feed_dict={xs: x_train_batch, ys: y_train_batch, rs: 0, ts: False})\n",
        "        validation_loss = sess.run(loss, feed_dict={xs: df_valid_feature, ys: arr_valid_label, rs:0, ts: False})\n",
        "        print(\"epoch\",i,\"validation loss=\", validation_loss,\"Train Loss=\",train_loss)\n",
        "        TrainLoss.append(train_loss)\n",
        "        ValidLoss.append(validation_loss)\n",
        "        epoch.append(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 validation loss= 2.8013132 Train Loss= 2.791047\n",
            "epoch 100 validation loss= 2.8203585 Train Loss= 2.8405666\n",
            "epoch 200 validation loss= 2.7788196 Train Loss= 2.7769907\n",
            "epoch 300 validation loss= 2.7413116 Train Loss= 2.734816\n",
            "epoch 400 validation loss= 2.7147086 Train Loss= 2.7149498\n",
            "epoch 500 validation loss= 2.676189 Train Loss= 2.684793\n",
            "epoch 600 validation loss= 2.634014 Train Loss= 2.6372194\n",
            "epoch 700 validation loss= 2.5991538 Train Loss= 2.6007788\n",
            "epoch 800 validation loss= 2.5763466 Train Loss= 2.5796664\n",
            "epoch 900 validation loss= 2.5488687 Train Loss= 2.5510323\n",
            "epoch 1000 validation loss= 2.5266716 Train Loss= 2.5366418\n",
            "epoch 1100 validation loss= 2.4978366 Train Loss= 2.4929013\n",
            "epoch 1200 validation loss= 2.4745886 Train Loss= 2.4685168\n",
            "epoch 1300 validation loss= 2.4535666 Train Loss= 2.4513965\n",
            "epoch 1400 validation loss= 2.4324355 Train Loss= 2.4416425\n",
            "epoch 1500 validation loss= 2.4027586 Train Loss= 2.3955843\n",
            "epoch 1600 validation loss= 2.3968272 Train Loss= 2.394375\n",
            "epoch 1700 validation loss= 2.3728352 Train Loss= 2.3732903\n",
            "epoch 1800 validation loss= 2.366292 Train Loss= 2.3598022\n",
            "epoch 1900 validation loss= 2.3346856 Train Loss= 2.3407307\n",
            "epoch 2000 validation loss= 2.3133068 Train Loss= 2.3190384\n",
            "epoch 2100 validation loss= 2.3037045 Train Loss= 2.295021\n",
            "epoch 2200 validation loss= 2.2926075 Train Loss= 2.2972713\n",
            "epoch 2300 validation loss= 2.2734294 Train Loss= 2.2625537\n",
            "epoch 2400 validation loss= 2.2599523 Train Loss= 2.2289143\n",
            "epoch 2500 validation loss= 2.246835 Train Loss= 2.2350757\n",
            "epoch 2600 validation loss= 2.2408419 Train Loss= 2.2482042\n",
            "epoch 2700 validation loss= 2.225199 Train Loss= 2.2382946\n",
            "epoch 2800 validation loss= 2.213806 Train Loss= 2.2109082\n",
            "epoch 2900 validation loss= 2.207562 Train Loss= 2.2064319\n",
            "epoch 3000 validation loss= 2.1941752 Train Loss= 2.1977222\n",
            "epoch 3100 validation loss= 2.1868427 Train Loss= 2.1817207\n",
            "epoch 3200 validation loss= 2.1694515 Train Loss= 2.164358\n",
            "epoch 3300 validation loss= 2.1605818 Train Loss= 2.1438968\n",
            "epoch 3400 validation loss= 2.1559765 Train Loss= 2.1594322\n",
            "epoch 3500 validation loss= 2.1428013 Train Loss= 2.144211\n",
            "epoch 3600 validation loss= 2.1383905 Train Loss= 2.1511998\n",
            "epoch 3700 validation loss= 2.1260297 Train Loss= 2.1214936\n",
            "epoch 3800 validation loss= 2.1168482 Train Loss= 2.0945976\n",
            "epoch 3900 validation loss= 2.1141794 Train Loss= 2.1067107\n",
            "epoch 4000 validation loss= 2.106803 Train Loss= 2.0991845\n",
            "epoch 4100 validation loss= 2.1024096 Train Loss= 2.108229\n",
            "epoch 4200 validation loss= 2.0967839 Train Loss= 2.1080687\n",
            "epoch 4300 validation loss= 2.088049 Train Loss= 2.072768\n",
            "epoch 4400 validation loss= 2.0819354 Train Loss= 2.0589528\n",
            "epoch 4500 validation loss= 2.0742245 Train Loss= 2.0555494\n",
            "epoch 4600 validation loss= 2.0642197 Train Loss= 2.0477\n",
            "epoch 4700 validation loss= 2.0537403 Train Loss= 2.0473502\n",
            "epoch 4800 validation loss= 2.046873 Train Loss= 2.0282717\n",
            "epoch 4900 validation loss= 2.047972 Train Loss= 2.0536947\n",
            "epoch 5000 validation loss= 2.0399406 Train Loss= 2.0272567\n",
            "epoch 5100 validation loss= 2.0392995 Train Loss= 2.0505\n",
            "epoch 5200 validation loss= 2.0360165 Train Loss= 2.0307872\n",
            "epoch 5300 validation loss= 2.0402236 Train Loss= 2.0088773\n",
            "epoch 5400 validation loss= 2.042411 Train Loss= 2.027367\n",
            "epoch 5500 validation loss= 2.0374415 Train Loss= 2.0473003\n",
            "epoch 5600 validation loss= 2.0304353 Train Loss= 2.0119317\n",
            "epoch 5700 validation loss= 2.034575 Train Loss= 2.0460331\n",
            "epoch 5800 validation loss= 2.0300927 Train Loss= 2.0167313\n",
            "epoch 5900 validation loss= 2.0302503 Train Loss= 2.0402837\n",
            "epoch 6000 validation loss= 2.0323565 Train Loss= 2.0158348\n",
            "epoch 6100 validation loss= 2.028569 Train Loss= 2.0022967\n",
            "epoch 6200 validation loss= 2.029289 Train Loss= 2.0206952\n",
            "epoch 6300 validation loss= 2.0372708 Train Loss= 2.052907\n",
            "epoch 6400 validation loss= 2.0363424 Train Loss= 2.0166347\n",
            "epoch 6500 validation loss= 2.0335314 Train Loss= 2.0646253\n",
            "epoch 6600 validation loss= 2.0328832 Train Loss= 2.0124793\n",
            "epoch 6700 validation loss= 2.032495 Train Loss= 2.0108454\n",
            "epoch 6800 validation loss= 2.0354376 Train Loss= 2.0332398\n",
            "epoch 6900 validation loss= 2.0403774 Train Loss= 2.0328107\n",
            "epoch 7000 validation loss= 2.0337362 Train Loss= 2.046114\n",
            "epoch 7100 validation loss= 2.0350661 Train Loss= 2.0271146\n",
            "epoch 7200 validation loss= 2.0359948 Train Loss= 2.0586283\n",
            "epoch 7300 validation loss= 2.0290012 Train Loss= 2.058802\n",
            "epoch 7400 validation loss= 2.0392716 Train Loss= 2.0385556\n",
            "epoch 7500 validation loss= 2.040775 Train Loss= 2.0473459\n",
            "epoch 7600 validation loss= 2.0437536 Train Loss= 2.052133\n",
            "epoch 7700 validation loss= 2.036068 Train Loss= 2.0485935\n",
            "epoch 7800 validation loss= 2.0348675 Train Loss= 2.0363204\n",
            "epoch 7900 validation loss= 2.026645 Train Loss= 2.041007\n",
            "epoch 8000 validation loss= 2.0346408 Train Loss= 2.030285\n",
            "epoch 8100 validation loss= 2.037078 Train Loss= 2.070723\n",
            "epoch 8200 validation loss= 2.0340562 Train Loss= 2.094656\n",
            "epoch 8300 validation loss= 2.032098 Train Loss= 2.011801\n",
            "epoch 8400 validation loss= 2.0276773 Train Loss= 1.9972742\n",
            "epoch 8500 validation loss= 2.0239506 Train Loss= 2.0581849\n",
            "epoch 8600 validation loss= 2.022183 Train Loss= 2.0132031\n",
            "epoch 8700 validation loss= 2.0261643 Train Loss= 2.0098538\n",
            "epoch 8800 validation loss= 2.0304348 Train Loss= 1.9889199\n",
            "epoch 8900 validation loss= 2.0321736 Train Loss= 2.017334\n",
            "epoch 9000 validation loss= 2.0416071 Train Loss= 2.090802\n",
            "epoch 9100 validation loss= 2.0393794 Train Loss= 2.0000875\n",
            "epoch 9200 validation loss= 2.0351632 Train Loss= 2.0496862\n",
            "epoch 9300 validation loss= 2.0396466 Train Loss= 2.0003257\n",
            "epoch 9400 validation loss= 2.0379667 Train Loss= 2.0224462\n",
            "epoch 9500 validation loss= 2.0276062 Train Loss= 2.0165906\n",
            "epoch 9600 validation loss= 2.030433 Train Loss= 2.0501485\n",
            "epoch 9700 validation loss= 2.0326924 Train Loss= 2.029788\n",
            "epoch 9800 validation loss= 2.0304782 Train Loss= 2.0168366\n",
            "epoch 9900 validation loss= 2.0377686 Train Loss= 2.0085232\n",
            "epoch 10000 validation loss= 2.0359352 Train Loss= 2.051144\n",
            "epoch 10100 validation loss= 2.0413978 Train Loss= 2.066422\n",
            "epoch 10200 validation loss= 2.0406861 Train Loss= 2.0502763\n",
            "epoch 10300 validation loss= 2.0434787 Train Loss= 2.0428927\n",
            "epoch 10400 validation loss= 2.0389042 Train Loss= 2.0303516\n",
            "epoch 10500 validation loss= 2.0416884 Train Loss= 2.0200741\n",
            "epoch 10600 validation loss= 2.0407648 Train Loss= 2.024349\n",
            "epoch 10700 validation loss= 2.0401344 Train Loss= 2.0709083\n",
            "epoch 10800 validation loss= 2.0347311 Train Loss= 2.0409708\n",
            "epoch 10900 validation loss= 2.0401943 Train Loss= 2.0250576\n",
            "epoch 11000 validation loss= 2.0412905 Train Loss= 2.0025704\n",
            "epoch 11100 validation loss= 2.0404003 Train Loss= 2.0568771\n",
            "epoch 11200 validation loss= 2.035956 Train Loss= 2.0364325\n",
            "epoch 11300 validation loss= 2.0289717 Train Loss= 2.022731\n",
            "epoch 11400 validation loss= 2.031094 Train Loss= 2.0677574\n",
            "epoch 11500 validation loss= 2.0364282 Train Loss= 2.0443397\n",
            "epoch 11600 validation loss= 2.034661 Train Loss= 2.0421846\n",
            "epoch 11700 validation loss= 2.0372791 Train Loss= 2.038487\n",
            "epoch 11800 validation loss= 2.040079 Train Loss= 2.0016599\n",
            "epoch 11900 validation loss= 2.0399375 Train Loss= 2.0572817\n",
            "epoch 12000 validation loss= 2.044696 Train Loss= 2.057282\n",
            "epoch 12100 validation loss= 2.0483317 Train Loss= 2.0481162\n",
            "epoch 12200 validation loss= 2.0451787 Train Loss= 2.0259602\n",
            "epoch 12300 validation loss= 2.0468569 Train Loss= 2.056905\n",
            "epoch 12400 validation loss= 2.0489404 Train Loss= 2.06118\n",
            "epoch 12500 validation loss= 2.0415144 Train Loss= 2.0395954\n",
            "epoch 12600 validation loss= 2.0421164 Train Loss= 2.0096862\n",
            "epoch 12700 validation loss= 2.0467827 Train Loss= 2.0471323\n",
            "epoch 12800 validation loss= 2.0436883 Train Loss= 2.0599682\n",
            "epoch 12900 validation loss= 2.045352 Train Loss= 2.0541682\n",
            "epoch 13000 validation loss= 2.0428724 Train Loss= 2.058511\n",
            "epoch 13100 validation loss= 2.043691 Train Loss= 2.0490544\n",
            "epoch 13200 validation loss= 2.043442 Train Loss= 2.04208\n",
            "epoch 13300 validation loss= 2.045364 Train Loss= 2.0876782\n",
            "epoch 13400 validation loss= 2.0420299 Train Loss= 1.9981937\n",
            "epoch 13500 validation loss= 2.0489335 Train Loss= 2.027351\n",
            "epoch 13600 validation loss= 2.0440369 Train Loss= 2.019733\n",
            "epoch 13700 validation loss= 2.041634 Train Loss= 2.012272\n",
            "epoch 13800 validation loss= 2.046573 Train Loss= 2.0986557\n",
            "epoch 13900 validation loss= 2.0546386 Train Loss= 2.1001189\n",
            "epoch 14000 validation loss= 2.053896 Train Loss= 2.0632405\n",
            "epoch 14100 validation loss= 2.063043 Train Loss= 2.0526373\n",
            "epoch 14200 validation loss= 2.0571146 Train Loss= 2.0299082\n",
            "epoch 14300 validation loss= 2.0556858 Train Loss= 2.0803404\n",
            "epoch 14400 validation loss= 2.0555832 Train Loss= 2.0276113\n",
            "epoch 14500 validation loss= 2.0609152 Train Loss= 2.0747538\n",
            "epoch 14600 validation loss= 2.0585513 Train Loss= 1.997865\n",
            "epoch 14700 validation loss= 2.0577006 Train Loss= 2.0872357\n",
            "epoch 14800 validation loss= 2.0511236 Train Loss= 2.0630927\n",
            "epoch 14900 validation loss= 2.0511932 Train Loss= 2.0037334\n",
            "epoch 15000 validation loss= 2.0570686 Train Loss= 2.025871\n",
            "epoch 15100 validation loss= 2.0518236 Train Loss= 2.064948\n",
            "epoch 15200 validation loss= 2.0536675 Train Loss= 2.02169\n",
            "epoch 15300 validation loss= 2.055226 Train Loss= 2.0722997\n",
            "epoch 15400 validation loss= 2.0543625 Train Loss= 2.033734\n",
            "epoch 15500 validation loss= 2.0543752 Train Loss= 2.0449305\n",
            "epoch 15600 validation loss= 2.0561635 Train Loss= 2.0990882\n",
            "epoch 15700 validation loss= 2.0540226 Train Loss= 2.1175137\n",
            "epoch 15800 validation loss= 2.053224 Train Loss= 1.9981787\n",
            "epoch 15900 validation loss= 2.0520375 Train Loss= 2.0289931\n",
            "epoch 16000 validation loss= 2.049279 Train Loss= 2.0384421\n",
            "epoch 16100 validation loss= 2.0499291 Train Loss= 2.0684724\n",
            "epoch 16200 validation loss= 2.0526838 Train Loss= 2.0336795\n",
            "epoch 16300 validation loss= 2.0529876 Train Loss= 2.044502\n",
            "epoch 16400 validation loss= 2.043425 Train Loss= 1.9809818\n",
            "epoch 16500 validation loss= 2.045453 Train Loss= 2.0414395\n",
            "epoch 16600 validation loss= 2.0431871 Train Loss= 2.057105\n",
            "epoch 16700 validation loss= 2.0373373 Train Loss= 2.0729194\n",
            "epoch 16800 validation loss= 2.0371156 Train Loss= 1.975136\n",
            "epoch 16900 validation loss= 2.0400348 Train Loss= 2.0796802\n",
            "epoch 17000 validation loss= 2.0374703 Train Loss= 2.0763316\n",
            "epoch 17100 validation loss= 2.0425408 Train Loss= 2.0300112\n",
            "epoch 17200 validation loss= 2.04741 Train Loss= 2.073656\n",
            "epoch 17300 validation loss= 2.051714 Train Loss= 2.0483446\n",
            "epoch 17400 validation loss= 2.0509725 Train Loss= 2.0261662\n",
            "epoch 17500 validation loss= 2.0519905 Train Loss= 2.0374856\n",
            "epoch 17600 validation loss= 2.0506957 Train Loss= 2.0442746\n",
            "epoch 17700 validation loss= 2.0547435 Train Loss= 2.0415285\n",
            "epoch 17800 validation loss= 2.0557218 Train Loss= 2.0340028\n",
            "epoch 17900 validation loss= 2.0561 Train Loss= 2.0469232\n",
            "epoch 18000 validation loss= 2.0577672 Train Loss= 2.062594\n",
            "epoch 18100 validation loss= 2.0557437 Train Loss= 2.0018337\n",
            "epoch 18200 validation loss= 2.0567687 Train Loss= 2.0083141\n",
            "epoch 18300 validation loss= 2.0576081 Train Loss= 2.032653\n",
            "epoch 18400 validation loss= 2.0561266 Train Loss= 2.058256\n",
            "epoch 18500 validation loss= 2.057333 Train Loss= 2.0734646\n",
            "epoch 18600 validation loss= 2.0539749 Train Loss= 2.0856357\n",
            "epoch 18700 validation loss= 2.0540051 Train Loss= 2.0250201\n",
            "epoch 18800 validation loss= 2.0521867 Train Loss= 2.0624702\n",
            "epoch 18900 validation loss= 2.0565367 Train Loss= 2.0796723\n",
            "epoch 19000 validation loss= 2.0589647 Train Loss= 2.0336843\n",
            "epoch 19100 validation loss= 2.0571067 Train Loss= 2.0893266\n",
            "epoch 19200 validation loss= 2.0514386 Train Loss= 2.0981133\n",
            "epoch 19300 validation loss= 2.0466723 Train Loss= 2.020029\n",
            "epoch 19400 validation loss= 2.0548007 Train Loss= 2.0601873\n",
            "epoch 19500 validation loss= 2.0486956 Train Loss= 1.9865282\n",
            "epoch 19600 validation loss= 2.051412 Train Loss= 2.071621\n",
            "epoch 19700 validation loss= 2.0556316 Train Loss= 2.0034065\n",
            "epoch 19800 validation loss= 2.053212 Train Loss= 2.0585427\n",
            "epoch 19900 validation loss= 2.0492144 Train Loss= 2.074834\n",
            "epoch 20000 validation loss= 2.0501611 Train Loss= 2.0896068\n",
            "epoch 20100 validation loss= 2.051509 Train Loss= 2.042444\n",
            "epoch 20200 validation loss= 2.0542023 Train Loss= 2.0691576\n",
            "epoch 20300 validation loss= 2.0476365 Train Loss= 2.0652645\n",
            "epoch 20400 validation loss= 2.0451317 Train Loss= 2.030709\n",
            "epoch 20500 validation loss= 2.040476 Train Loss= 2.0573502\n",
            "epoch 20600 validation loss= 2.0421598 Train Loss= 2.0135906\n",
            "epoch 20700 validation loss= 2.0450175 Train Loss= 2.0784926\n",
            "epoch 20800 validation loss= 2.0469599 Train Loss= 2.046273\n",
            "epoch 20900 validation loss= 2.043813 Train Loss= 2.0510361\n",
            "epoch 21000 validation loss= 2.045851 Train Loss= 2.0595977\n",
            "epoch 21100 validation loss= 2.0489948 Train Loss= 2.0513973\n",
            "epoch 21200 validation loss= 2.050334 Train Loss= 2.0296736\n",
            "epoch 21300 validation loss= 2.0508595 Train Loss= 2.0440392\n",
            "epoch 21400 validation loss= 2.0488672 Train Loss= 2.02736\n",
            "epoch 21500 validation loss= 2.0504842 Train Loss= 2.0606713\n",
            "epoch 21600 validation loss= 2.0542028 Train Loss= 2.087929\n",
            "epoch 21700 validation loss= 2.054509 Train Loss= 2.0326896\n",
            "epoch 21800 validation loss= 2.0535185 Train Loss= 2.0503347\n",
            "epoch 21900 validation loss= 2.0534644 Train Loss= 2.042533\n",
            "epoch 22000 validation loss= 2.052121 Train Loss= 2.0588293\n",
            "epoch 22100 validation loss= 2.0554023 Train Loss= 2.0368109\n",
            "epoch 22200 validation loss= 2.050366 Train Loss= 2.0399048\n",
            "epoch 22300 validation loss= 2.0514274 Train Loss= 2.0557234\n",
            "epoch 22400 validation loss= 2.051009 Train Loss= 2.1093738\n",
            "epoch 22500 validation loss= 2.0517921 Train Loss= 2.0365825\n",
            "epoch 22600 validation loss= 2.0520418 Train Loss= 2.04829\n",
            "epoch 22700 validation loss= 2.0536494 Train Loss= 2.0446463\n",
            "epoch 22800 validation loss= 2.0570056 Train Loss= 2.097944\n",
            "epoch 22900 validation loss= 2.053354 Train Loss= 2.0311248\n",
            "epoch 23000 validation loss= 2.0544355 Train Loss= 2.04701\n",
            "epoch 23100 validation loss= 2.0523646 Train Loss= 2.0500553\n",
            "epoch 23200 validation loss= 2.0490103 Train Loss= 2.060875\n",
            "epoch 23300 validation loss= 2.04716 Train Loss= 2.0204825\n",
            "epoch 23400 validation loss= 2.048348 Train Loss= 2.0407894\n",
            "epoch 23500 validation loss= 2.046833 Train Loss= 2.0643518\n",
            "epoch 23600 validation loss= 2.043997 Train Loss= 2.053562\n",
            "epoch 23700 validation loss= 2.047668 Train Loss= 2.0664763\n",
            "epoch 23800 validation loss= 2.0466077 Train Loss= 2.0805953\n",
            "epoch 23900 validation loss= 2.044537 Train Loss= 2.0650935\n",
            "epoch 24000 validation loss= 2.0459363 Train Loss= 2.0426266\n",
            "epoch 24100 validation loss= 2.0451376 Train Loss= 2.0721817\n",
            "epoch 24200 validation loss= 2.046605 Train Loss= 2.086315\n",
            "epoch 24300 validation loss= 2.0454874 Train Loss= 2.045292\n",
            "epoch 24400 validation loss= 2.046535 Train Loss= 2.0130107\n",
            "epoch 24500 validation loss= 2.0467718 Train Loss= 2.0297108\n",
            "epoch 24600 validation loss= 2.0445745 Train Loss= 2.0332346\n",
            "epoch 24700 validation loss= 2.0450087 Train Loss= 2.0352132\n",
            "epoch 24800 validation loss= 2.043851 Train Loss= 2.0699277\n",
            "epoch 24900 validation loss= 2.0479586 Train Loss= 2.0553854\n",
            "epoch 25000 validation loss= 2.0494623 Train Loss= 2.0304146\n",
            "epoch 25100 validation loss= 2.052276 Train Loss= 2.0490885\n",
            "epoch 25200 validation loss= 2.0501637 Train Loss= 2.0469384\n",
            "epoch 25300 validation loss= 2.0501556 Train Loss= 2.043773\n",
            "epoch 25400 validation loss= 2.0516138 Train Loss= 2.0713668\n",
            "epoch 25500 validation loss= 2.052626 Train Loss= 2.0358431\n",
            "epoch 25600 validation loss= 2.0537617 Train Loss= 2.038545\n",
            "epoch 25700 validation loss= 2.0525732 Train Loss= 2.0578473\n",
            "epoch 25800 validation loss= 2.0524793 Train Loss= 2.0569165\n",
            "epoch 25900 validation loss= 2.0503223 Train Loss= 2.0453005\n",
            "epoch 26000 validation loss= 2.0503318 Train Loss= 2.0507243\n",
            "epoch 26100 validation loss= 2.048631 Train Loss= 2.0481846\n",
            "epoch 26200 validation loss= 2.049618 Train Loss= 2.043999\n",
            "epoch 26300 validation loss= 2.0482755 Train Loss= 2.0453784\n",
            "epoch 26400 validation loss= 2.0463672 Train Loss= 2.0567672\n",
            "epoch 26500 validation loss= 2.0483704 Train Loss= 2.0253437\n",
            "epoch 26600 validation loss= 2.0496495 Train Loss= 2.0216458\n",
            "epoch 26700 validation loss= 2.0480902 Train Loss= 2.0400355\n",
            "epoch 26800 validation loss= 2.0478306 Train Loss= 2.027281\n",
            "epoch 26900 validation loss= 2.0492105 Train Loss= 2.0485373\n",
            "epoch 27000 validation loss= 2.0459883 Train Loss= 2.066202\n",
            "epoch 27100 validation loss= 2.0472813 Train Loss= 2.0391364\n",
            "epoch 27200 validation loss= 2.0468068 Train Loss= 2.0527167\n",
            "epoch 27300 validation loss= 2.0482945 Train Loss= 2.060064\n",
            "epoch 27400 validation loss= 2.0467038 Train Loss= 2.0461562\n",
            "epoch 27500 validation loss= 2.0461347 Train Loss= 2.054157\n",
            "epoch 27600 validation loss= 2.0449722 Train Loss= 2.0288038\n",
            "epoch 27700 validation loss= 2.0466805 Train Loss= 2.0452077\n",
            "epoch 27800 validation loss= 2.0460753 Train Loss= 2.08187\n",
            "epoch 27900 validation loss= 2.0433505 Train Loss= 2.0431013\n",
            "epoch 28000 validation loss= 2.0443077 Train Loss= 2.030023\n",
            "epoch 28100 validation loss= 2.0452907 Train Loss= 2.0469978\n",
            "epoch 28200 validation loss= 2.046471 Train Loss= 2.028359\n",
            "epoch 28300 validation loss= 2.046095 Train Loss= 2.045395\n",
            "epoch 28400 validation loss= 2.0462527 Train Loss= 2.0623362\n",
            "epoch 28500 validation loss= 2.0447235 Train Loss= 2.048108\n",
            "epoch 28600 validation loss= 2.0454175 Train Loss= 2.04316\n",
            "epoch 28700 validation loss= 2.0445101 Train Loss= 2.0394707\n",
            "epoch 28800 validation loss= 2.0439813 Train Loss= 2.0482352\n",
            "epoch 28900 validation loss= 2.0445046 Train Loss= 2.055312\n",
            "epoch 29000 validation loss= 2.0462432 Train Loss= 2.040775\n",
            "epoch 29100 validation loss= 2.0467594 Train Loss= 2.0969133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiTFEYJhCY_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% plot loss\n",
        "fig = plt.figure()\n",
        "# 111代表在subplot圖中的位置\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(epoch[1:], TrainLoss[1:], c='b', label='TrainLoss', linewidth=0.5)\n",
        "ax.plot(epoch[1:], ValidLoss[1:], c='r', label='ValidLoss', linewidth=0.5)\n",
        "leg = plt.legend()\n",
        "# get the lines and texts inside legend box\n",
        "leg_lines = leg.get_lines()\n",
        "leg_texts = leg.get_texts()\n",
        "# bulk-set the properties of all lines and texts\n",
        "plt.setp(leg_lines, linewidth=6)\n",
        "plt.setp(leg_texts, fontsize='x-large')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EC2RnGUIBVv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add ops to save the variables.\n",
        "saver = tf.train.Saver()\n",
        "save_path = saver.save(sess, path+\"StoredModel/v4_model.ckpt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-mJQftU6Qzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#Restore Model\n",
        "sess = tf.Session()\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(sess, path+\"StoredModel/v4_model.ckpt\")\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsFkHtIN92-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arr_train_pred = sess.run(prediction, feed_dict={xs: df_train_feature, ys: arr_train_label, rs: 0, ts: False})\n",
        "arr_valid_pred = sess.run(prediction, feed_dict={xs: df_valid_feature, ys: arr_valid_label, rs: 0, ts: False})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu0ZYmxYAvmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_accuracy = accuracy(arr_train_label, arr_train_pred)\n",
        "print(\"train_accuracy = \", train_accuracy*100,\"%\")\n",
        "valid_accuracy = accuracy(arr_valid_label, arr_valid_pred)\n",
        "print(\"valid_accuracy = \", valid_accuracy*100,\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS6p12DdSShB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}